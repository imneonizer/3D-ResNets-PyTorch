{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e150463b-8b24-42fb-bb7c-656e01345afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nitin/github/3D-ResNets-PyTorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists('models') or os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189814b6-4592-411c-9e93-1f62f81f14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Pytorch Model: /home/nitin/github/kinetics/images/results/save_200.pth\n",
      "generated simplified onnx model:kinetics_r50_122x122_test.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx as torch_onnx\n",
    "from models import resnet\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "def export_model(\n",
    "    model_path,\n",
    "    onnx_path,\n",
    "    n_classes,\n",
    "    shape,\n",
    "    batch_size=1,\n",
    "    seq_length=16,\n",
    "    model_depth=50,\n",
    "    n_input_channels=3,\n",
    "    shortcut_type=\"B\",\n",
    "    conv1_t_size=7,\n",
    "    conv1_t_stride=1,\n",
    "    no_max_pool=False,\n",
    "    widen_factor=1.0):\n",
    "    \n",
    "    # A model class instance\n",
    "    model = resnet.generate_model(\n",
    "        model_depth=model_depth,\n",
    "        n_classes=n_classes,\n",
    "        n_input_channels=n_input_channels,\n",
    "        shortcut_type=shortcut_type,\n",
    "        conv1_t_size=conv1_t_size,\n",
    "        conv1_t_stride=conv1_t_stride,\n",
    "        no_max_pool=no_max_pool,\n",
    "        widen_factor=widen_factor\n",
    "     )\n",
    "\n",
    "    # Load the weights from a file\n",
    "    print(f\"loading Pytorch Model: {model_path}\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    # Load the weights now into a model net architecture\n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()    \n",
    "    dummy_input = torch.randn(batch_size, 3, seq_length, shape[0], shape[1], requires_grad=True)\n",
    "    out = model(dummy_input)\n",
    "    torch_onnx.export(\n",
    "        model, dummy_input, onnx_path,\n",
    "        export_params=True, opset_version=10, do_constant_folding=True,\n",
    "        input_names = ['input'], output_names = ['output'],\n",
    "        dynamic_axes={'input' : {0 : 'batch_size'},'output' : {0 : 'batch_size'}}\n",
    "    )\n",
    "\n",
    "    # use onnxsimplify to reduce reduent model.\n",
    "    input_shapes = {\"input\": list(dummy_input.shape)}\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    model_simp, check = simplify(onnx_model, dynamic_input_shape=True, input_shapes=input_shapes)\n",
    "    assert check, \"Simplified ONNX model could not be validated\"\n",
    "    onnx.save(model_simp, onnx_path)\n",
    "    print(f\"generated simplified onnx model: {onnx_path}\")\n",
    "\n",
    "\n",
    "export_model(\n",
    "    model_path=\"/home/nitin/github/kinetics/images/results/save_200.pth\",\n",
    "    onnx_path=\"kinetics_r50_122x122_test.onnx\",\n",
    "    shape=(112, 112),\n",
    "    n_classes=5,\n",
    "    seq_length=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78dbdbd1-ccdf-4639-bd71-f2106091e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX as inference backend\n",
      "Using weight: kinetics_r50_122x122_test.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Resnet3DONNX():\n",
    "    import onnxruntime as ort\n",
    "\n",
    "    def __init__(self, model_path, classes, shape=(112, 112), seq_length=16, mean=None, std=None):\n",
    "        print(f'Using ONNX as inference backend')\n",
    "        print(f'Using weight: {model_path}')\n",
    "        self.model_path = model_path\n",
    "        self.classes = classes\n",
    "        self.shape = shape\n",
    "        self.seq_length = seq_length\n",
    "        self.mean = mean or [0.4345, 0.4051, 0.3775]\n",
    "        self.std = std or [0.2768, 0.2713, 0.2737]\n",
    "        self.ort_session = self.ort.InferenceSession(self.model_path)\n",
    "        self.input_name = self.ort_session.get_inputs()[0].name\n",
    "    \n",
    "    def softmax(self, x, dim=0):\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape((1, -1))\n",
    "        max_x = np.max(x, axis=1).reshape((-1, 1))\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\n",
    "    \n",
    "    def preprocess(self, batch):\n",
    "        batch_inputs = []\n",
    "        for images in batch:\n",
    "            inputs = []\n",
    "            for img in images:\n",
    "                img = cv2.resize(img, self.shape)\n",
    "                img = ((img/255.0) - self.mean) / self.std\n",
    "                inputs.append(img)\n",
    "            batch_inputs.append(inputs[:self.seq_length])\n",
    "        return np.array(batch_inputs).transpose(0,4,1,2,3)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.ort_session.run(None, {self.input_name: inputs})[0]\n",
    "    \n",
    "    def inference(self, batch):\n",
    "        inputs = self.preprocess(batch)\n",
    "        # batch,channel,frames,h,w\n",
    "        outputs = self(inputs.astype(np.float32))\n",
    "        outputs = self.softmax(outputs, dim=1)\n",
    "\n",
    "        predictions = []\n",
    "        for batch_output in outputs:\n",
    "            p = sorted(list(zip(self.classes, batch_output)), key=lambda x: x[1], reverse=True)\n",
    "            predictions.append(p)\n",
    "        return predictions\n",
    "\n",
    "onnx_model = Resnet3DONNX(\n",
    "    \"kinetics_r50_122x122_test.onnx\",\n",
    "    ['archery', 'bowling', 'marching', 'flying_kite', 'high_jump'],\n",
    "    (112, 112)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e5befa-e5cf-4278-b88c-68a3339c614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import cv2\n",
    "\n",
    "def load_frames(path, limit=16):\n",
    "    frames = []\n",
    "    for path in glob.glob(path+\"/*\")[:limit]:\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(img)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027b890a-4aa4-4795-ba80-8825395d72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = load_frames(\"/home/nitin/github/kinetics/images/val/bowling/4JxH3S5JwMs_000003_000013\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15fed468-b5ce-4872-9236-ceae206cc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bowling', 0.28196692),\n",
       " ('marching', 0.24497674),\n",
       " ('high_jump', 0.21826945),\n",
       " ('flying_kite', 0.13246316),\n",
       " ('archery', 0.12232374)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.inference([frames])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
